<!-- 
<span class='anchor' id='research-interests'></span>
# Research Interests
- **(α) Agentic reinforcement learning** that strengthens reasonning, planning, and generalization for LLM-based agents.
- **(β) Human-aligned evaluation** frameworks and **scalable benchmarks** for LLM agents.
- **(γ) Scalable data synthesis pipelines** that mix supervised, reinforcement, and self-improvement signals for continual agent growth.

<u>I see reinforcement learning as the key to unlocking the full agentic potential of large language models.</u> To get there, we need **(α)** richer training paradigms that fuse supervised, reinforcement, and self-improvement signals; rigorous, **(β)** human-aligned evaluation suites to keep agents grounded; and **(γ)** scalable data pipelines that deliver the fuel these systems need to continually improve. Aligned with these agendas, I have worked on agentic foundation models ([OpenCUA](https://opencua.xlang.ai/)), agent data synthesis ([AgentTrek](https://agenttrek.github.io/)), agent evaluation ([OSWorld-Verified](https://xlang.ai/blog/osworld-verified)), and related initiatives.

<div align="center">
  <img src="{{ "/research_image.png" | relative_url }}" alt="Research directions diagram" width="600">
</div>

*High-level view of my research interest across agentic reinforcement learning(interaction), evaluation, and data synthesis.* -->